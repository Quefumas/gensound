# -*- coding: utf-8 -*-
"""
Created on Thu Aug 22 20:44:57 2019

@author: Dror
"""

timeit(setup="import numpy as np", number=10000, stmt="")
================ CORE ======================

large-scale future goals:
. make available a host of adjustable Filters and EQs
* make them parametric as well
V CF, BiTransforms and negative shift
* better support for FM synthesis, also other kinds of synthesis
* simulate guitar amps, distortion and other effects
V upsample/downsample
* interfacing improved playback options
? better musical logic support for intonations and melodies
? drum patterns

fundamental improvements to do:

* fix fades
V find syntax for upsampling/slowing down/ etc.
V find how to put a signal after another signal
V and to repeat signals
V make signals/trnasforms empty skeletons
    so that one may reuse them, or define their own to be reused
    (make sure they store no audio data! apart from RAW)
* keep track of all dangerous floats (when converting from time to samples)
  perhaps maintain a convention that time is in floats, samples in ints
  (that would mean that to slice samples we need to fill in the channels,
  otherwise it will infer wrongly)
no doubt we still need direct After function; putting signals in arrays is not always appropriate
perhaps we should use + as after and & as mix?
V specifying byte_width should be relevant only when exporting, not on mixdown.
  this is because Audio objects don't need to carry along sound quality information,
  except for sample rate.
  Also consider that synthesizing a Signal, then bouncing it into Audio
  and then into a Raw (for example in incremental composition) entails choosing
  byte width just to ignore it again.

next stages:
V curves controlling Gain, Amplitude
* AmpFreq(?), Fade(?), then go over all other transforms
V fix curve discontinuity
V finish panning

V parametrization
V controlled glissandos
V tracks stuff
V fix panning
V ADSR
* TODO fix streak
* pan stereo (we can do a specialized transform for this, using a MultiCurve
  for opening angle and general angle, but is this actually more convenient?
  isn't doing it manually more natural?)
V supply one example of custom panning scheme
* Signal as Curve?
V crossfades

V negative shifts

V user-defined curves
V user-defined panning schemes [futuristic tho]!!!!

* crossfade for concat???
Sine() | CF | Triangle() ???


sample where MyReverb = Reverb(dry=0.4, time=3)... etc

V TODO: test why squares get negative all the time
* TODO: Audio.mixdown vs. Signal.mixdown.
  Probably should change name of Audio.mixdown - it's just a preparation
  for output of the already-mixed data. possibly should not even be called
  by Signal.mixdown.
* reconsider audio.conform. perhaps should only conform channels.
  only used in audio.mix, concat, and in the tentative strange multiply/convolve
V TODO enable concating signal to 0 (supported already?), or to empty Signal()?
enforce "no such thing as seconds" rule! but remember that frequency is measured in Hz...

= Paramterization TODOs:
V Panning
V some problems when concatting two together
* merge with Signal
* for signal have duration irrelevant when frequency is a curve (since it has duration already)
* instead of flatten use __float??

= curves continuing to infinity issues:
* for frequency, that would mean the signal duration can be specified to be
    more than frequency duration. if that is done, we'd expect the signal
    to stop early as well if the duration is shorter than the curve
* or, for frequency have the duration setting ignored; if you want the pitch
    to keep going, add Constant() at the end of the curve.
    in glissando madrigal settings that makes more sense.
* for amplitude/gain/fade/pan envelope, these are transforms and not signals!
    so we expect the curve to affect only the transform, but then stay at
    the place it stopped.

* to consider: 
    SineCurve(frequency=6, depth=3, baseline=330, duration=5e3) | Constant(...)
    may cause problems due to sine not completing whole cycle
    we do not want to force user to compute this, what could be better is 
    SineCurve(frequency=6, depth=3, baseline=330, duration=5e3) | CF | Constant(...)
    i.e. design a kind of cross-fade that makes these continuous
    #####* or (better?) solution: use a Curve instead of 

- Curve*Signal interpreted as Amplitude*Signal? [i.e. replacing float]

* s = WAV(african)[5e3:10e3] takes different parts of african according
  to the sample rate given when realising. bug or feature?
* s = WAV(african)[15e3:25e3:2] works as expected. bug or feature?

? make number of eventual channels seep up to master signal?
  or conclude it by examining the tree? just so we can iterate over channels and do sums?
  
? if signal has 2 subscripts and the 2nd [time] is in ints - maybe have it refer to samples
  instead of time. this allows for more surgical placement of new signals.
  is this possible? or wanted?
  otoh, keep in mind that we can't actually change value of single samples via Signal
  and maybe it should stay that way (though we can replace it with a signal of our choosing or mute it)



Add sweepSine signal


play/file IO functionality as method under Audio?

V when accepting frequency argument, allow strings of form "C4", mainly for ease
also maybe "C" defaults to "C4", and "C4+23" lets you play with cents
- perhaps even let the user define package-wide custom interpretations for frequencies
so they can decide how to input them themselves, or decide on intonation etc.
- perhaps make it comma-separable "C,E,G" jointly creates several Signals and then mixes them togetherr

Make Fade() be interpreted as in/out by the direction from which it is applied?!??!!
on the one hand this is intuitive and useful.
on the other hand this breaks commutativity property
and thus contaminates a kind of purity in the existing syntax
though come to thing of it, multiplication never was commutative in the first place

? how to implement "specialized" Transforms/Signals, e.g. we have a general-purpose FIR
  where the user specifies the weights, and we wish to create a LowPass filter that
  receives a cut-off frequency and computes the arguments to the FIR.
  one way is to do:
  def LowPass(cutoff): # masquerading as class (PEP 8: "Names that are visible to the user as public parts of the API should follow conventions that reflect usage rather than implementation.")
      return FIR(...)

  another way: <--- this should be it probably
  class LowPass(FIR):
      def __init__(self, cutoff):
          super().__init__(compute weights...)

? addition of FIRs/IIRs? so several may be computed at one go. if that really works that way

* add chord/sequence features to easy frequency notation (e.b. "Ab,C,Eb" or "Ab C Eb")

* infer end phases between concatenated pitched signals (would that be a feature or a bug?)

* consider: feeding "r"/None as frequency for Oscillator renders rest (useful for programming melodies)

* maybe let Signal.concat accept list of Signals as well (instead of only multiple arguments)

* consider allowing Audio float subscripts (does interpolation behind the scenes)

=============== Syntactical stuff =============

=== Fades ===

* what would it take to infer type of Fade by direction of multiplication?
    Fade*signal (fade in) vs. signal*Fade (fade out)
    since the Fade has to know which it is, it must figure that out either
    on applying or on realising.


* set package-wide default fade length and shape?

* Fade(curve="linear/exp/.../default")

=== Signal - Number operations ===

* add DC: sig + 4 
* concat silence: sig | 4e3

are these not implemented yet?

=== Infer durations/phases/others ===

* s[5e3:] += Sine(1e3) # even though LHS is long, this should only affect 1e3
  (probably like this already)
* s[5e3:5.5e3] += Sine(1e3) # TODO what happens here? what should happen?
* s[5e3:] += Sine(freq) # TODO can I infer RHS duration from LHS slice?
  # it is not trivial to even know LHS duration. what if it is given explicitly?
  # infer in call time or mixdown?
* very complicated inference (crazy idea):
  s[5e3:10e3] = Sine(f1) | Sine(f2) | Sine(f3, duration=3e3)
  # figures out RHS should amount to 5e3, 3e3 is already specified,
  # divided the remaining 2e3 betweeen the first 2.
  # as cool as this is, it's kind of a backwards way to write code;
  # won't any sane (hah) developer prefer to give explicit duration,
  # maybe using a constant or something?
* Sine("G3 E4") # TODO automatically collapse to Sine("G3") + Sine("E4")?
* Sine("G3, E4") # automatically interpret as melody (same duration for all?)
  we can take this way further. Also why comma, space instead of +, | etc?
  also this is troublesome for melodies as applying transform to this (think ADSR)
  will only apply to the concatted, not necessarily as useful.
* in general, omitting freq or duration argument will take the previous
  value given to this argument anywhere in the code.
  but that would be very bad style and shouldn't be encouraged.
  would that be such a great advantage to have? not so sure.
* Sine(440, 1e3) | Triangle(550, 1e3)
  # this is pure concatenation of two oscillators,
  # we would like (by default?) to have the ending phase of the first
  # transferred to the second
  what is the correct mechanism to do this?
  - one way is when overloading "|" to see whether both are oscillators,
    if so then grab the end phase from the 1st and copy to the 2nd.
    note that if the 1st oscillator's duration is to be inferred,
    that must happen prior to the phase being inferred.
  - another way is when realise()ing Sequence,
    where it would either copy the phase into the 2nd oscillator phase,
    or we do it as an argument to osc's realise() (which sounds wrong).
* WAV(...) + 5
  # if 5 is to be interpreted as DC, need to infer duration of WAV/whatever


give each Signal or Transform the power to traverse the tree and decide for itself!

=== Resampling ===
* Should this be using a Transform? or be an Audio/Raw method instead?
* Use cases:
  - Loading WAVs with both 44.1kHz and 48kHz together, one of these will have to change.
    This is for raw audio only.
  - Slow/hasten synthesized signal, changing pitch. This sounds like fun, but
    for generated signals we choose durations and frequencies ourselves,
    so we can just change these parameters instead of doing lossy (and slow)
    interpolation. OTOH, with resampling the syntax is so much shorter.
  - Not sure in what way, but this can be combined with a curve that makes
    the original signal slow down or hasten in a controlled, time-dependent way.
    This would be super cool.
* If applied as transform to Raw, how does it infer the original sample rate
  for inferring the 1st argument? The Raw may have been already transformed
  and thus lost its Raw properties.
* Maybe we should always require 2 arguments?
* Maybe just one argument which is the ratio actually?
* Maybe also accept it as a method for Raw or Audio?
* Or maybe as an argument to WAV, to resample on loading?
* But wait - Audio always (?) has a sample rate, so by the time the Transform
  is realise()d, it is *always* possible to infer the 1st argument.
  Therefore there is never a reason to enable 2 arguments.
  So we can write w = WAV(kushaura)*Resample(32000) and go on our way.

Syntax jam:

raw_sig *= Resample(32000) # Resamples to 32kHz from whatever the Raw data had
                    # (recall that Raw has inherent original recommend sample rate)
any_sig *= Resample(39000, 32000) # Resamples from supposed original sample rate into new one
                    # none of these actually have to be real sample rates,
                    # thought it won't make too much sense otherwise
any_sig *= Resample(32k, 8k, gs.LINEAR_INTERPOLATION)
any_sig *= Resample(32k, 8k, method="linear")
any_sig *= Stretch(curve or constant) # if constant, this is identical to resample


Further thoughts:
* thinking about the sample rate carried along by Audio...
* when we have Raw audio, it's normal for it to remember somehow its original
  sample rate. but once we use mixdown, that should be completely ignored.
* as for synthesized audio, it can only make sense for it to keep the mixdown rate.
* that means that once audio is mixed-down, only the sample rate argument should be kept.
* so what then about resample? we need to keep in mind the above 2 use-cases
  and realise that they are actually quite different.
  for the first (make sample rate conform), the user knows what the destination
  sample rate should be, and they're going to use that for mixdown().
  they don't necessarily care what the original file sample rate was.
* for the second use-case, the name 'resample' is misleading:
  resampling is a process we apply to a signal so it can be played back
  in a different sample rate and still result in a similar output.
  if we do time-stretch, we are actually not interested at all in what sample
  rate difference this would mean, and we would actually prefer the Transform
  to behave the same no matter what rate we use on mixdown.
  In the suggested syntax Resample(dest_rate), suppose we eventually do mixdown
  using dest_rate, then the Resample ended up doing nothing,
  even though we wanted to use it to stretch.
  so for this use case, what is interesting is only the rate of stretching,
  or alternatively, the resulting duration of the signal.
  and these should be the only possible arguments.
* To sum up, this should be covered using 2 different functionalities,
  one for Resampling Raws only, given a destination sample rate,
  and one for Stretching any Signal, given rate or target duration.
  Since they are used for different purposes and receive different arguments,
  I suggest having separate names.

* For the 2nd, Stretch could be adequate. Brainstorming for a moment:
  stretch, haste/slow, fit, TimeStretch, Smear, Prolong, Speed, AtSpeed,
  ToTime, ToDuration, Duration,
* it is not out of the question to have different Transforms, one for
  stretching by ratio, and one for fitting into particular time duration.
* and a major question. should the 1st use case be implemented as
  an Audio function or as a Transform?
  Maybe not so difficult to answer: this operation should only ever be applied
  to Raw signals, thus it would be very shitty as a Transform, as these
  are inherently applicable to all Signals.
  If it only makes sense for basically one Signal, we can't make it a Transform,
  which also prohibits the use of the * operator.
  So function it is.

* one more note: even though we can give as argument the interpolation method used,
  'linear', 'quadratic', 'nn' or whatever,
  the actual default value should not (exactly) be 'quadratic' but rather
  'default' - indicating to use package-wide setting, which defaults to quadratic,
  but may be changed by a one-liner.
  this may be more useful if we can also use this to let the user inject their own
  interpolators.



====== Negative Shifts and CF ======
* Shifted signals when mixed down should perhaps not be fully generated on their own
  with the padded zeros; but rather placed into the mix properly, to save the memory
  allocation of the padding.
* negative shifted signals when concatted should coincide into the previous signal:
  s = Sine(duration=3e3) | Square(duration=3e3)*Shift(-1e3)
  if this goes back beyond the beginning of the signal, the entire mixdown
  should be considered Shifted backwards
* and the shifted property, should all Signals be allowed to have it?
  can be it always safely ignored, except when realising Sequences? and Mixes?
  also it must be conveyed to whatever Signal you're being transformed/mixed into
* when using Sequence, have keyworded argument for BiTransform to be applied
  in the middle (automatic overlap or automatic CF)
* define class TransformChain which is array of transforms, we may multiply it
  by Transforms, adding them to the list.
  when a signal is multiplied by it, it breaks down and the transforms in the chain
  are added individually.
* define class BiTransform which accepts two Transforms/TransformChains, L and R
  and may be concated on both sides to Signals only.
* upon concat Signal | BiTransform, the L transform chain is applied to Signal,
  and the last item in the sequence contains the TransformChain R
* if BiTransform is right-concatted with Signal (by the above points this
  should have been illegal, as it would have been first left-concatted into
  a Signal and dissolved within it), it can also default to disppearing,
  or alternatively ignoring L part. The only advantage for this is the syntactical
  ease when concatting via for loop, so we may prepend each concatted signal
  by a CF, even though we start empty.
* when concatting Signal into Sequence, check to see if the last item
  on the list is a TransformChain. if so, replace it by the concatted Signal
  times that transform chain.
* alternative implementation: leave the BiTransform as an item in Sequence,
  and only upon realise() apply the transforms. pbbly better to stop in the middle
  and break the BiTransform down, so that realise would only be called once
  for each signal in Sequence.
  the only advantage here is that when pretty-printing the signal tree,
  we get to see the BiTransform.
* how about more complex BiTransforms? that actually let both signals interact?
  that would mean that on Sequence.realise() we would have to *look ahead*
  before realising, or perhaps realise the first, then upon bumping into
  the BiTrans we would skip it, realise whatever's afterwards, then come back
  and apply L,R to these results AND let BiTrans concat on its own.
  and this coould cause problems if the R signal has yet another bitransform
  afterwards. i think we leave this for user custom designs.
* think about future placements in grid, i.e. filling up drum samples.
  should we use an At/Shift function? or concat and compute the overlaps?
  perhaps we can give everything a Shift and give an extra argument instructing
  what to do with overlaps (could be "mix as is", or CF, or maybe something else
  such as fade the first one out just in time for the appearance of the second?)

implementation thoughts of audio.shift:
* especially comes into play when conforming audios, mixing and concatting

* reconsider audio.conform. maybe redefine it.

* note that mix/concat signals reverts to audios so should be fine

* when mixing 2 audios:
  they are actually represented as:
      (0,b) with shift a,
      (0,d) with shift c.
  and supposed to become:
      (0,f) with shift e.
  
  we interpret them as ranges on the number axis:
      (a,b+a), (c,d+c), (e,f+e)
  we need then e = min(a,c)
  and f+e = max(b+a, d+c),
  therefore f = max(b+a, d+c) - e
  
  but in particular, if we wish to extend(a,b+a) to include the other,
  we need it to first become (e, b+e)
  by pre-padding by (a - e).
  then into (e, f+e)
  by post-padding by (f+e)-(b+a)
  note that when d+c <= b+a, the above is just (b+a)-(b+a)=0
  
  (sanity check: total length is now indeed (a-e)+b+(f+e)-(b+a) = f)
  
  then finally, we add the 2nd one inside in the correct place,
  which on the axis is c, and relatively it is c-e.
  
  pseudo-code:
    # conforming
    e = min(one.shift, two.shift)
    f = max(one.shift+one.length(), two.shift+two.length())
    one.prepad(one.shift-e)
    one.postpad(f+e - (one.shift+one.length()))
    one.shift = e
    # mixing
    one[:,two.shift-one.shift:] += two
    
 
 * when concatting:
   can also revert to the previous case with some adaptation
   or vice versa?
   
   if two.shift < 0:
     if -two.shift > one.length:
       diff = -two.shift - one.length
       one.prepend(diff)
       one.shift -= diff
     first[:,two.shift:] += two[:,:-two.shift]
     first.append(two[:,-two.shift:])
   elif two.shift >= 0:
     first.postpad(two.shift)
     first.append(two)

 * to use concat code for mix, first perform:
     two.shift -= one.length
   to use mix code to perform concat, first perform:
     two.shift += one.length

-===- Panning -===-
(pan law should be class variable for Pan, so we can set it once)

use cases:
V take mono signal and pan it into several channels
V apply amplitude on individual channels (this is covered already no?)
V take stereo signal and make it span the space differently
    relatively easy to derive from the 1st note above

mono -> multiple - how do we do it?
V provide n curves to indicate particular gains/levels
  but note that we can just expand mono to multi then apply individual envelops
V pre-define a mapping R -> R^n, and then just give one function t->R


suppose listener in middle of triangle, we can use complex number to give exact source of sound
    to_channels = lambda x: ()
does it mean we need to enable 2d curves??? or just combining them
it would be easier to use polar coordinates instead of complex numbers tho

# what happens if i do s[0]*=Pan()? how many channels do  i end up with??
the above has undefined behavior and throws an error - for good reason.

-===- Transforms --
use '.' to apply transforms?
Sine().Pan().Gain().Shift() + ...
In that case can't use amplitude as float


* stereo.L ? # overriding set/getattr is a messy, will doo later

* taking stereo signal and panning it in stereo (i.e. make it take up -100L to -10L)
    new_stereo = stereo*Pan(panLaw=default, panL, panR)

preferably have as many of these done by syntax as possible.

=-=== parametrization syntax =====

? can we do piece-wide parametrization? for example make the ADSRs change with
  time over many notes, by a curve that started well before the signals themselves.
  the problem is this would require each signal to understand when it starts
  in absolute time. this should be workable but needs care.


things that would be paramterized:
* Dbs/amplitudes
* EQ data
* Pan
* frequency or phase (integral of frequency)

* define Curve.__call__? makes it easier to define panning schemes

here the object is functions of time (also of channels??)

# perhaps make Signal inherit from "Time-dependent" class, which is also used for time params

or we can define as checkpoints:
pan = line((0,0), (50,1e3), (50,3e3), (100,5e3))

* another possibility:
    curve = 1e3*const(5) | 5e3*line(220,440)
    (though this would go against mergin signals and curves)

* cumulative vs. absolute time durations - important considerations
* also need to do last value continuing to infinity.
  but that would make duration not clear.
  (wait: this depends on whether the param is freq or amplitude)
  may be better (i,e for envelopes) to have the duration of the note specified,
  and the curve only applying for its own duration.
  i.e. note of 6 secs, with the envelope applying for 100ms
* to allow integration we may also let predefined curves supply their own integral
behaves like an iterator/iterable?

-------- Summary of operators to support nicely:

& mix
+ concat
* apply transform
** repeat !!!
* scalar? amplitude

t = s[:,3e3:5e3]
s[:, 3e3:5e3] *= Reverse()

| float (concat with silence of float duration)
+ float (add DC/constant of amount float)

what about order of operations?
do we want concat or mix first?

=-====
perhaps supply a length-of operator?
wav = (x:=Sine()) + Square().Shift(duration=x.length())

* also, we may want after in conjunction with [negative] shift.
* do we want before?




important: can transforms extend the signal on their own?
there is the transform EXtend which does so.
sliced transforms need to be careful not to move stuff around when they do
artifical extensions (i.e. reverb etc)

important: are transforms powerful enough to do flexible reverbs?
i guess they can do so by direct manipulation on audio,
but it also seems natural to implement transforms by 

=================Other stuff =======================

- functions scheduled for examination:
    - signal.realise
    - amplitude.realise

- once we solve the big ones above, probably the next stage is having a full-fledged
EQ and Compressor.


V perhaps have transforms multipliable by each other to create chains,
which can then be added in bulk to a signal.
 the purpose is that the user can employ any order of multiplications to get
the desired result
for that we may need transforms to copy themselves on apply



- if num_channels, sample rate etc. are needed during signal realisation,
we can perrhaps pass a param dictionary instead. if we can deal
with only sample rate, thats good too.



perhaps ensure that all signals ARE empty shells
otherwise we cannot construct a signal for reuse


===================== SIGNALS ====================

subclass of Raw that stores the buffer of a mixed-down Audio!
this is also great for incremental composition, you can put aside previous results

implement len() of signals

compare between signals using length?

let users supply phase function instead of frequency function,
to allow them to spare the manual integration.

perhaps have all signals store their audio in a cache, since transformed
signales may well be used multiple times

class Majchord(Mix):
    def __init__(self, base_freq, sig, duration):
        self.signals = [sig(frequency=base_freq, duration=duration),
                        sig(frequency=base_freq*semi(4), duration=duration)]
                        ...

also easier representations of melody,
i.e. give semitones and signal type, durations,
and it automatically gnerates a Sequence

=================== TRANSFORMS ===================

- cut and delete middle of signal

- do transforms *between* signals, i.e. binary transforms.
  for example: crossfade, or slice something from the middle
  for this we can perhaps implement the same concat syntax:
  Signal | Binary-transform | Signal.

implement __eq__? to see if class and arguments are the same

force quantize? suppose i have byte width 2 but want only 1000 levels of amplitude

Transform: Right/Left
changes to Stereo and puts in appropriate plae
Also:
Stereo(pan = [-1,1])

* mute according to pattern
* add extend


perhaps allow multiplication by float vector,, one per channel

for multiple channels - enable user defined directions!
not necessarily linear, can be used to compose for strangely placed 3d speakers


have a pan transform that allows for a single function that defines direction,
with the addition of a power law


downsample/upsample



allow negative shifts? (which can cancel previous positive shifts,
or lose the beginning of the audio)

experiment: reverse compress.
high amps get lowered down, low amps get powered up


possibly faster reverb:
for i, s in enumerate(audio.audio):
    audio.audio[i] += temp
    temp *= 0.2
    temp += audio.audio[i]


============== enhanced transforms/effect =====
now that we have apply encapsulated by realise,
perhaps we can create a new Superclass of transforms,
which upon realise actually receieves the relevant
Signal (not audio), does actual shit to it and then generates()!


================== ANALYZE =======================

DC balance - analyse and correct


implement in analyze.py a function to test frequency response of transforms
by applying them to the impulse and then computing DFT magniuteds


for continuous DFT:
https://en.wikipedia.org/wiki/Short-time_Fourier_transform

================= OTHER CAPABILITIES =============
interface with MIDI!

allow realtime interactions

look at Audacity list of effects (and source code) for more transform ideas

================ DEBUG & STABILITY ===========================
- Audio.__add__ should not affect audio.
perhaps implement iadd instead.

- Audio.conform should not return anything

- when mixdown takes more than a few seconds, print a record of how long it took
(preferably to the log as well)

ensure that signals and transforms are applid according to the same order
note that this may be annoying when using __radd__ etc


summarize all places where floating number convert to int
so the user knows exactly all the places of possible inaccuracy






TODO
apparently it really doesn't copy:
ones = lambda n: tuple([1.0 for x in range(n)])
    w = WAV(filename)
    wav = sum([(1-8/10)*w*Shift(duration=100*x)*Average_samples(weights=ones(2*x+1)) for x in range(5)])
    #wav += 
    
    audio = wav.mixdown(sample_rate=44100, byte_width=2)
    play_Audio(audio, is_wait=True)
this downsamples!

the problem actgually is not in copying the audio; its the fact
that multiplying signals with transforms changes the original signal
its the signal that may need to be copied anew
perhaps just use audio cache is better


============== ????? =============

np view vs. copy
http://www.jessicayung.com/numpy-views-vs-copies-avoiding-costly-mistakes/
https://stackoverflow.com/questions/4370745/view-onto-a-numpy-array

perhaps define "10s" to be interpreted as 10*1000

alias "sum" as "mix"

for export_test, get the actual code of the calling function,
and store it as metadata to the WAV!

come back to sound synthesis book one day


add frequency/pitch class calculus
as well as time measurement calculus (add beat, add 1/16, add 1/3 etc)
which can be realised at generate-time according to selected tempo

musical functionality to pick random note from distribution
distributions can be midi ranges, scales, whatever

connect to VSTs to use inside the code!


dB = 20 * log10(amplitude)



amplitude = 14731 / 32767
          = 0.44

dB = 20 * log10(0.44)
   = -7.13









