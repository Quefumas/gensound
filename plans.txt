# -*- coding: utf-8 -*-
"""
Created on Thu Aug 22 20:44:57 2019

@author: Dror
"""


================ CORE ======================

fundamental improvements to do:

V find syntax for upsampling/slowing down/ etc.
V find how to put a signal after another signal
V and to repeat signals
V make signals/trnasforms empty skeletons
    so that one may reuse them, or define their own to be reused
    (make sure they store no audio data! apart from RAW)
V slice signals
V take back to mono etc.
V translate fadeins etc into db
V solve thing about float vs. int (and enable ints as floats all around)
V signal addition code ignores transforms applied to sums?
* keep track of all dangerous floats (when converting from time to samples)
no doubt we still need direct After function; putting signals in arrays is not always appropriate
perhaps we should use + as after and & as mix?

next stages:
* parametrization
V controlled glissandos
V tracks stuff
* fix panning
* ADSR
* TODO fix streak

* negative shifts?

* crossfade for concat???
Sine() | CF | Triangle() ???


V TODO: test why squares get negative all the time
* TODO: Audio.mixdown vs. Signal.mixdown.
  Probably should change name of Audio.mixdown - it's just a preparation
  for output of the already-mixed data. possibly should not even be called
  by Signal.mixdown.
V TODO enable concating signal to 0 (supported already?), or to empty Signal()?
enforce "no such thing as seconds" rule! but remember that frequency is measured in Hz...

Paramterization TODOs:
* Panning
* some problems when concatting two together
* merge with Signal
* for signal have duration irrelevant when frequency is a curve (since it has duration already)

* to consider: 
SineCurve(frequency=6, depth=3, baseline=330, duration=5e3) | Constant(...)
may cause problems due to sine not completing whole cycle
we do not want to force user to compute this, what could be better is 
SineCurve(frequency=6, depth=3, baseline=330, duration=5e3) | CF | Constant(...)
i.e. design a kind of cross-fade that makes these continuous

- Curve*Signal interpreted as Amplitude*Signal? [i.e. replacing float]

* s = WAV(african)[5e3:10e3] takes different parts of african according
  to the sample rate given when realising. bug or feature?
* s = WAV(african)[15e3:25e3:2] works as expected. bug or feature?

- make number of eventual channels seep up to master signal?
  or conclude it by examining the tree? just so we can iterate over channels and do sums?
====== Syntactical stuff =======

----
use '.' to apply transforms?
Sine().Pan().Gain().Shift() + ...
In that case can't use amplitude as float

--===- Channel stuff synatax ===

t = Signal()
t[0:2,:] = Sine() # automatically switches into stereo

=-=

t = Sine()
t[1] = Square() # automatically adds another channel?

=-=

mono = t[0] + t[1] #?

mono = sum(t) #?

mono = t.mono() # mono in what way?
mono = t * mono()?
=-=

V
t = Sine()[0:2] # automatically expands the sine into stereo
# this syntax would have the advantage that it does not require
# defining t in advance!

=-=

MonoSignal*Stereo(Lfunc, Rfunc)?
Mono*Stereo(35L)?

common operations:
* mapping mono to several channels (exact copies)
V   stereo = mono[0:2] ?
    stereo = (0.5,1)*mono[0:2] ?
    # TODO aside: signal subscript to distinguish ints and floats for channels and time?
    
    Signal function?
    Sine().to_channels(3)
* mapping mono into one of several channels (adding empty channels)
V   stereo = mono[0:2]
    stereo[0:2] *= (0,1) ?
V moving channels around (i.e. RePan)
    stereo = stereo[0:2:-1] ? # works already
    
    triple = Signal()[0:3]
    triple[0] = one
    triple[1] = two
    triple[2] = three
    
    what if just:
    triple = Signal()
    triple[0] = one
    triple[2] = three # automatically opens 3rd channel
* applying fades/gains to channels in separate
    stereo[0] *= fade()
    stereo.L ? # overriding set/getattr is a messy, will doo later

* taking mono signal and moving it around in stereo (or more) by some function
    need as input a panLaw, function mapping an argument to levels in all channels
    and then do for a mono:
    pan = package.range(-100, 100, 5e3)
    stereo = mono*Pan(panLaw=packg.def_panLaw, pan)
* taking stereo signal and panning it in stereo (i.e. make it take up -100L to -10L)
    new_stereo = stereo*Pan(panLaw=default, panL, panR)

preferably have as many of these done by syntax as possible.

=-=== parametrization syntax =====

things that would be paramterized:
* Dbs/amplitudes
* EQ data
* frequency or phase (integral of frequency)

here the object is functions of time (also of channels??)

pan = -1*one(5secs) | line(-100L, 100R, 5secs) | one(5secs)
also do like sigmoid (1/(e^-x +1)) or tanh/arctan between two values, or log from -infty etc.   
# perhaps make Signal inherit from "Time-dependent" class, which is also used for time params

what about:
line(-100, 100)*time(5 secs)
line(from=-100, to=100, duration=5e3)

or we can define as checkpoints:
pan = line((0,0), (50,1e3), (50,3e3), (100,5e3))

maybe fancir:

pan = (0,-100) | line | (3e3, -50) | line | (5e3, -50)

or we can omit the "line":
pan = (0,-100) | (3e3, -50) | (5e3, -50)

or maybe:
pan = -100 | 5e3*line | 100 | 10e3*sigmoid(beta=3) | 50

* another possibility:
curve = 1e3*const(5) | 5e3*line(220,440)
(though this would go against mergin signals and curves)

* cumulative vs. absolute time durations - important considerations
* looks kinda sick to do this alternating concat between values and functions
* also need to do last value continuing to infinity.
  but that would make duration not clear.
  (wait: this depends on whether the param is freq or amplitude)
  may be better (i,e for envelopes) to have the duration of the note specified,
  and the curve only applying for its own duration.
  i.e. note of 6 secs, with the envelope applying for 100ms
* to allow integration we may also let predefined curves supply their own integral
behaves like an iterator/iterable?

=--- need to implement what for curves? =---
typically they would be iterated over (think of time-dependent convolution),
or multiplied
perhaps easiest for now is to make them flatten themselves to arrays as needed


-------- Summary of operators to support nicely:

& mix
+ concat
* apply transform
** repeat !!!
* scalar? amplitude

t = s[:,3e3:5e3]

s[:, 3e3:5e3] *= Reverse()

s[:, 3e3:5e3] = Reverse()

s[:, 3e3:5e3] += Reverse()


s= WAV(african)
what about order of operations?
do we want concat or mix first?

=-====
perhaps supply a length-of operator?
wav = (x:=Sine()) + Square().Shift(duration=x.length())

* also, we may want after in conjunction with [negative] shift.
* do we want before?




important: can transforms extend the signal on their own?
there is the transform EXtend which does so.
sliced transforms need to be careful not to move stuff around when they do
artifical extensions (i.e. reverb etc)

important: are transforms powerful enough to do flexible reverbs?
i guess they can do so by direct manipulation on audio,
but it also seems natural to implement transforms by 

============================

- functions scheduled for examination:
    - signal.realise
    - amplitude.realise

- once we solve the big ones above, probably the next stage is having a full-fledged
EQ and Compressor.


- perhaps have transforms multipliable by each other to create chains,
which can then be added in bulk to a signal.
 the purpose is that the user can employ any order of multiplications to get
the desired result
for that we may need transforms to copy themselves on apply



- if num_channels, sample rate etc. are needed during signal realisation,
we can perrhaps pass a param dictionary instead. if we can deal
with only sample rate, thats good too.



perhaps ensure that all signals ARE empty shells
otherwise we cannot construct a signal for reuse


===================== SIGNALS ====================

subclass of Raw that stores the buffer of a mixed-down Audio!
this is also great for incremental composition, you can put aside previous results

implement len() of signals

compare between signals using length?

let users supply phase function instead of frequency function,
to allow them to spare the manual integration.

perhaps have all signals store their audio in a cache, since transformed
signales may well be used multiple times

class Majchord(Mix):
    def __init__(self, base_freq, sig, duration):
        self.signals = [sig(frequency=base_freq, duration=duration),
                        sig(frequency=base_freq*semi(4), duration=duration)]
                        ...

also easier representations of melody,
i.e. give semitones and signal type, durations,
and it automatically gnerates a Sequence

=================== TRANSFORMS ===================

- cut and delete middle of signal

- do transforms *between* signals, i.e. binary transforms.
  for example: crossfade, or slice something from the middle
  for this we can perhaps implement the same concat syntax:
  Signal | Binary-transform | Signal.

implement __eq__? to see if class and arguments are the same

force quantize? suppose i have byte width 2 but want only 1000 levels of amplitude

Transform: Right/Left
changes to Stereo and puts in appropriate plae
Also:
Stereo(pan = [-1,1])

* mute according to pattern
* add extend


perhaps allow multiplication by float vector,, one per channel

for multiple channels - enable user defined directions!
not necessarily linear, can be used to compose for strangely placed 3d speakers


have a pan transform that allows for a single function that defines direction,
with the addition of a power law


downsample/upsample



allow negative shifts? (which can cancel previous positive shifts,
or lose the beginning of the audio)

experiment: reverse compress.
high amps get lowered down, low amps get powered up


possibly faster reverb:
for i, s in enumerate(audio.audio):
    audio.audio[i] += temp
    temp *= 0.2
    temp += audio.audio[i]


============== enhanced transforms/effect =====
now that we have apply encapsulated by realise,
perhaps we can create a new Superclass of transforms,
which upon realise actually receieves the relevant
Signal (not audio), does actual shit to it and then generates()!


================== ANALYZE =======================

DC balance - analyse and correct


implement in analyze.py a function to test frequency response of transforms
by applying them to the impulse and then computing DFT magniuteds


for continuous DFT:
https://en.wikipedia.org/wiki/Short-time_Fourier_transform

================= OTHER CAPABILITIES =============
interface with MIDI!

allow realtime interactions


================ DEBUG & STABILITY ===========================
- Audio.__add__ should not affect audio.
perhaps implement iadd instead.

- Audio.conform should not return anything

- when mixdown takes more than a few seconds, print a record of how long it took
(preferably to the log as well)

ensure that signals and transforms are applid according to the same order
note that this may be annoying when using __radd__ etc


summarize all places where floating number convert to int
so the user knows exactly all the places of possible inaccuracy






TODO
apparently it really doesn't copy:
ones = lambda n: tuple([1.0 for x in range(n)])
    w = WAV(filename)
    wav = sum([(1-8/10)*w*Shift(duration=100*x)*Average_samples(weights=ones(2*x+1)) for x in range(5)])
    #wav += 
    
    audio = wav.mixdown(sample_rate=44100, byte_width=2)
    play_Audio(audio, is_wait=True)
this downsamples!

the problem actgually is not in copying the audio; its the fact
that multiplying signals with transforms changes the original signal
its the signal that may need to be copied anew
perhaps just use audio cache is better


============== ????? =============

np view vs. copy
http://www.jessicayung.com/numpy-views-vs-copies-avoiding-costly-mistakes/
https://stackoverflow.com/questions/4370745/view-onto-a-numpy-array

perhaps define "10s" to be interpreted as 10*1000

alias "sum" as "mix"

for export_test, get the actual code of the calling function,
and store it as metadata to the WAV!

come back to sound synthesis book one day


add frequency/pitch class calculus
as well as time measurement calculus (add beat, add 1/16, add 1/3 etc)
which can be realised at generate-time according to selected tempo

musical functionality to pick random note from distribution
distributions can be midi ranges, scales, whatever

connect to VSTs to use inside the code!


dB = 20 * log10(amplitude)



amplitude = 14731 / 32767
          = 0.44

dB = 20 * log10(0.44)
   = -7.13









